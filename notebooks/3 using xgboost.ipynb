{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../src/train/requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `pip`, restart the notebook kernel by going to the menu at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The MLflow server is here: {}'.format(os.environ['MLFLOW_TRACKING_URI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiment results will be logged into the \"Dev\" experiment\n",
    "experiment_name = os.environ.get('MLFLOW_EXPERIMENT_NAME', 'Dev')\n",
    "\n",
    "e = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not e:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "else:\n",
    "    experiment_id = e.experiment_id\n",
    "\n",
    "print('Experiment ID: {}'.format(experiment_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = mlflow.start_run(experiment_id=experiment_id, run_name = 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc.api\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "# The credidcard.csv versioning metadata is kept in a git repository in os.environ['DATA_REPO'].\n",
    "# We are pulling version 1.0 of the data and the data itself is from a S3 bucket that is returned\n",
    "# by dvc.api.get_url()\n",
    "resource_url = dvc.api.get_url(\n",
    "    path='creditcard.csv',\n",
    "    repo=os.environ['DATA_REPO'],\n",
    "    rev='v1.0')\n",
    "\n",
    "print(\"Data is from this S3 bucket:\\n{}\".format(resource_url))\n",
    "\n",
    "# Pandas doens't support endpoint_url\n",
    "# https://github.com/pandas-dev/pandas/pull/29050\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': os.environ['S3_ENDPOINT_URL']})\n",
    "df = pd.read_csv(fs.open(resource_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding features with the highest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_corr(param, n):\n",
    "    class_corr = df.corr()[param].sort_values(ascending=False)\n",
    "    list_class = []\n",
    "    for i in features:\n",
    "        if(np.abs(class_corr[i]) >= n): \n",
    "           list_class.append(i)\n",
    "    return list_class\n",
    "# Select features with correlation higher than 0.1 (positive correlation) or lower than -0.1 (negative correlation)\n",
    "selected_features = most_corr('Class', 0.1)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Building XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, \\\n",
    "    average_precision_score, plot_precision_recall_curve, f1_score, auc, \\\n",
    "    roc_curve, roc_auc_score, confusion_matrix, accuracy_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "mlflow.log_param('RANDOM_SEED', RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.3\n",
    "mlflow.log_param('TEST_SIZE', TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.drop('Class',1) , dataset['Class'], test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "\n",
    "# Set xgboost parameters\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.039\n",
    "params['max_depth'] = 2\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.9\n",
    "params['eval_metric'] = 'auc'\n",
    "params['random_state'] = RANDOM_SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autolog results using mlflow\n",
    "import mlflow.xgboost\n",
    "mlflow.xgboost.autolog() \n",
    "\n",
    "baseline = xgb.train(params, \n",
    "                dtrain, \n",
    "                1000, \n",
    "                watchlist, \n",
    "                early_stopping_rounds=50, \n",
    "                maximize=True, \n",
    "                verbose_eval=50)\n",
    "\n",
    "y_baseline = baseline.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_baseline = baseline.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(y_test, y_proba_baseline)\n",
    "mlflow.log_metric('average_precision', average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_proba_baseline > THRESHOLD))\n",
    "print('\\n')\n",
    "print('AUC: {}%'.format(roc_auc_score(y_test, y_proba_baseline)))\n",
    "print('Precision-Recall: {}'.format(average_precision))\n",
    "\n",
    "rpt = classification_report(y_test, y_proba_baseline > THRESHOLD, output_dict=True)\n",
    "for lbl in ['0', '1']:\n",
    "    mlflow.log_metric(lbl + '_recall', rpt[lbl]['recall'])\n",
    "    mlflow.log_metric(lbl + '_f1_score', rpt[lbl]['f1-score'])\n",
    "    mlflow.log_metric(lbl + '_precision', rpt[lbl]['precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC - ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_baseline)\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.fill_between(fpr, tpr, color='skyblue', alpha=0.3)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unbalanced data\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba_baseline)\n",
    "plt.plot(recall, precision, marker='.', label='{} (AP={:.4f})'.format(baseline.__class__.__name__, average_precision))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('2-class Precision-Recall curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"Normal\", \"Fraud\"]\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_proba_baseline>THRESHOLD) # rows = truth, cols = prediction    \n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(df_cm, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_proba_baseline>THRESHOLD)\n",
    "mlflow.log_metric('accuracy', accuracy)\n",
    "\n",
    "plt.title('Accuracy: {:.4f}'.format(accuracy))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}